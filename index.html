<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Cam Ring Detector</title>
    <!-- Tailwind CSS for modern styling -->
    <script src="https://cdn.tailwindcss.com"></script>
    <!-- TensorFlow.js library -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest/dist/tf.min.js"></script>
    <style>
        /* A simple loader animation */
        .loader {
            border: 5px solid #f3f3f3;
            border-top: 5px solid #3b82f6; /* blue-500 */
            border-radius: 50%;
            width: 50px;
            height: 50px;
            animation: spin 1s linear infinite;
        }
        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }
    </style>
</head>
<body class="bg-gray-100 flex items-center justify-center min-h-screen">

    <div class="container mx-auto p-4 max-w-lg">
        <div class="bg-white rounded-2xl shadow-lg p-6 text-center">
            <h1 class="text-2xl md:text-3xl font-bold text-gray-800 mb-2">Cam Ring Detector</h1>
            <p class="text-gray-500 mb-4">Point your camera at a ring and click "Detect".</p>

            <!-- Webcam Feed -->
            <div class="relative w-full aspect-square bg-gray-200 rounded-xl shadow-inner overflow-hidden mx-auto">
                <video id="webcam" autoplay playsinline class="w-full h-full object-cover"></video>
                <!-- Status display for loading, errors, etc. -->
                <div id="status" class="absolute inset-0 flex items-center justify-center bg-white bg-opacity-80">
                    <div class="text-center">
                        <div id="loader" class="loader mx-auto"></div>
                        <p id="status-text" class="mt-4 text-lg font-semibold text-gray-700">Initializing...</p>
                    </div>
                </div>
            </div>

            <!-- Action Button -->
            <button id="capture-button" class="w-full bg-blue-500 hover:bg-blue-600 text-white font-bold py-3 px-4 rounded-lg mt-6 transition-all duration-300 shadow-md disabled:bg-gray-400 disabled:cursor-not-allowed">
                Detect
            </button>

            <!-- Prediction Output -->
            <div class="mt-4 text-xl font-semibold h-12 flex items-center justify-center text-gray-700">
                <p id="prediction-output">Prediction will appear here</p>
            </div>
        </div>
    </div>

    <script>
        // --- CONFIGURATION ---
        // 1. UPDATE THE MODEL PATH
        // This path points to the 'model.json' file inside your 'web_model' folder.
        const MODEL_URL = './web_model/model.json';

        // 2. UPDATE THE CLASS NAMES
        // IMPORTANT: These MUST be in the exact same order as in your training script.
        // Check your 'class_names.txt' file to be sure.
        const CLASSES = [
            'OK_Top', 'NG_Top', 'OK_Bottom', 'NG_Bottom' 
            // Example: 'Class1', 'Class2', 'Class3', etc.
        ];

        const IMAGE_SIZE = 224; // Must match the input size of your model (e.g., 224 for MobileNet)

        // --- DOM ELEMENTS ---
        const video = document.getElementById('webcam');
        const statusContainer = document.getElementById('status');
        const statusText = document.getElementById('status-text');
        const loader = document.getElementById('loader');
        const captureButton = document.getElementById('capture-button');
        const predictionOutput = document.getElementById('prediction-output');

        let model = null;

        /**
         * Main function to initialize the model and webcam
         */
        async function initialize() {
            statusText.textContent = "Loading Model...";
            captureButton.disabled = true;

            try {
                // Load the TensorFlow.js model
                model = await tf.loadLayersModel(MODEL_URL);
                // Warm up the model to make the first prediction faster
                tf.tidy(() => {
                    model.predict(tf.zeros([1, IMAGE_SIZE, IMAGE_SIZE, 3]));
                });
                console.log('Model loaded successfully.');

                // Setup the webcam
                await setupWebcam();
                console.log('Webcam setup complete.');
                
                // Everything is ready
                statusContainer.style.display = 'none';
                captureButton.disabled = false;

            } catch (error) {
                console.error('Initialization failed:', error);
                statusText.textContent = "Error: Could not start. Check console.";
                loader.style.display = 'none';
            }
        }

        /**
         * Sets up the webcam stream
         */
        async function setupWebcam() {
            return new Promise((resolve, reject) => {
                // Check for webcam support
                if (navigator.mediaDevices && navigator.mediaDevices.getUserMedia) {
                    navigator.mediaDevices.getUserMedia({ video: { facingMode: 'environment' } }) // Prefer back camera
                        .then(stream => {
                            video.srcObject = stream;
                            video.addEventListener('loadeddata', resolve, false);
                        })
                        .catch(error => {
                            console.error("Webcam access denied or not available.", error);
                            reject(new Error("Please grant webcam permission and refresh."));
                        });
                } else {
                    reject(new Error("Your browser does not support webcam access."));
                }
            });
        }

        /**
         * Captures a frame, predicts, and displays the result
         */
        async function predict() {
            if (!model) {
                console.log("Model not loaded yet.");
                return;
            }

            predictionOutput.textContent = "Analyzing...";
            
            // Use tf.tidy to automatically manage and clean up tensors
            const predictionResult = tf.tidy(() => {
                // 1. Capture a "snapshot" from the video and convert to a tensor
                const imgTensor = tf.browser.fromPixels(video);

                // 2. Pre-process the image: resize, normalize, and add a batch dimension
                const processedTensor = imgTensor
                    .resizeBilinear([IMAGE_SIZE, IMAGE_SIZE])
                    .div(255.0)
                    .expandDims(0);

                // 3. Make the prediction
                const predictions = model.predict(processedTensor);
                
                // 4. Get the results
                const scores = predictions.dataSync(); // Get prediction scores as a JS array
                const predictedIndex = predictions.argMax(1).dataSync()[0]; // Get the index of the highest score
                
                return {
                    className: CLASSES[predictedIndex],
                    confidence: (scores[predictedIndex] * 100).toFixed(2)
                };
            });
            
            // 5. Display the final result
            predictionOutput.innerHTML = `Prediction: <strong class="text-blue-600">${predictionResult.className}</strong> (${predictionResult.confidence}%)`;
        }

        // --- Event Listeners ---
        captureButton.addEventListener('click', predict);

        // Start the application
        initialize();
    </script>
</body>
</html>
